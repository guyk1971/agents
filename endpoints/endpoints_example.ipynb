{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d86f85d",
   "metadata": {},
   "source": [
    "# Endpoint Access\n",
    "The goal of this notebook is to examine the ability of a kernel agent to generate kernels that are memory bw limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd273f",
   "metadata": {},
   "source": [
    "## Setup and Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22527694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guy/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/guy/code/study/git/guyk1971/gen_ai_sb/endpoints/.env'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "root_dir = os.path.dirname(os.path.abspath(''))\n",
    "root_dir\n",
    "sys.path.append(root_dir)\n",
    "from endpoints import MODEL_NAME_TO_ID,ask_frontier_llm,ask_nim_llm\n",
    "env_path=os.path.join(root_dir,'endpoints','.env')\n",
    "env_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf25e59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'clds35': 'claude-3-5-sonnet-20241022', 'clds37': 'claude-3-7-sonnet-20250219', 'clds4': 'claude-sonnet-4-20250514', 'cldo4': 'claude-opus-4-20250514', 'gpt-4o': 'gpt-4o-20241120', 'gpt-4o-mini': 'gpt-4o-mini-20240718', 'gpt-4-turbo': 'gpt-4-turbo-20240409', 'o1-preview': 'o1-preview-20240912', 'o1-mini': 'o1-mini-20240912', 'o1': 'o1-20241217', 'o3mini': 'o3-mini-20250131', 'llama3.3': 'nvdev/meta/llama-3.3-70b-instruct', 'dsr1': 'nvdev/deepseek-ai/deepseek-r1'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown\n",
    "# Remember to load the environment variables. You should have the Groq API Key in there :)\n",
    "load_dotenv(env_path)\n",
    "api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpt=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "perlab_api_key = os.getenv(\"PERFLAB_API_KEY\")\n",
    "\n",
    "print(api_version)\n",
    "print(MODEL_NAME_TO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dc761",
   "metadata": {},
   "source": [
    "### Quick sanity check and usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72eda3",
   "metadata": {},
   "source": [
    "#### Frontier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frontier models \n",
    "model_id=MODEL_NAME_TO_ID['clds35']\n",
    "client = AzureOpenAI(azure_endpoint=azure_endpt,\n",
    "                     api_version=api_version,\n",
    "                     api_key=api_key)\n",
    "\n",
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "\n",
    "mergesort_code = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    model=model_id\n",
    ").choices[0].message.content\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": mergesort_code\n",
    "    }\n",
    ")\n",
    "display_markdown(mergesort_code, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=MODEL_NAME_TO_ID['clds35']\n",
    "user_prompt = \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "system_prompt = \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "mergesort_code = ask_frontier_llm(system_prompt,user_prompt,model_id)\n",
    "display_markdown(mergesort_code,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299ae08",
   "metadata": {},
   "source": [
    "## Accessing through Langchain \n",
    "\n",
    "Make sure you have the required libraries installed : \n",
    "```bash\n",
    "! pip install langchain langgraph langchain_openai \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpt,\n",
    "    api_version=api_version,\n",
    "    api_key=api_key,\n",
    "    model=MODEL_NAME_TO_ID['clds35'],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Test the setup\n",
    "response = llm.invoke(\"Hello! Are you working?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=MODEL_NAME_TO_ID['clds35']\n",
    "user_prompt_template = \"\"\"Generate a Python implementation of the {algo_name} algorithm\"\"\"\n",
    "system_prompt_template = \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_template),\n",
    "    (\"human\", user_prompt_template)\n",
    "])\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpt,\n",
    "    api_version=api_version,\n",
    "    api_key=api_key,\n",
    "    model=model_id,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "chain = template | llm\n",
    "\n",
    "response = chain.invoke({\"algo_name\": \"Merge Sort\"})\n",
    "\n",
    "# Clean up the response, removing markdown code fences\n",
    "clean_code = response.content.strip().replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "print(clean_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d0909",
   "metadata": {},
   "source": [
    "#### NIM models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=MODEL_NAME_TO_ID['llama3.3']\n",
    "user_prompt = \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "system_prompt = \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "mergesort_code = ask_nim_llm(system_prompt,user_prompt,model_id)\n",
    "display_markdown(mergesort_code,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080eb0a0",
   "metadata": {},
   "source": [
    "# Accessing local (HF) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-32B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-32B\",torch_dtype=torch.bfloat16,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to generate the prompt\n",
    "prompt = \"\"\n",
    "for message in generation_chat_history:\n",
    "    if message[\"role\"] == \"system\":\n",
    "        prompt += f\"System: {message['content']}\\n\"\n",
    "    elif message[\"role\"] == \"user\":\n",
    "        prompt += f\"User: {message['content']}\\n\"\n",
    "    elif message[\"role\"] == \"assistant\":\n",
    "        prompt += f\"Assistant: {message['content']}\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "prompt_length = inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more direct\n",
    "inputs2 = tokenizer.apply_chat_template(\n",
    "    generation_chat_history,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "prompt_length = inputs2.shape[1]  # Number of tokens in the prompt\n",
    "# inputs = {k: v.to(model.device) for k, v in inputs2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs2,\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc57601",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = output[0][prompt_length:]\n",
    "\n",
    "# 5. Decode only the new tokens\n",
    "assistant_response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b9480",
   "metadata": {},
   "source": [
    "## Testing the generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb35e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Sorts a list using the Merge Sort algorithm.\n",
    "    \n",
    "    Args:\n",
    "        arr (list): The list to be sorted.\n",
    "        \n",
    "    Returns:\n",
    "        list: A new sorted list.\n",
    "    \"\"\"\n",
    "    # Base case: if the array has one element or is empty, it's already sorted\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    \n",
    "    # Divide the array into two halves\n",
    "    mid = len(arr) // 2\n",
    "    left_half = merge_sort(arr[:mid])  # Recursively sort the left half\n",
    "    right_half = merge_sort(arr[mid:])  # Recursively sort the right half\n",
    "    \n",
    "    # Combine the sorted halves\n",
    "    return merge(left_half, right_half)\n",
    "\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merges two sorted lists into a single sorted list.\n",
    "    \n",
    "    Args:\n",
    "        left (list): The first sorted list.\n",
    "        right (list): The second sorted list.\n",
    "        \n",
    "    Returns:\n",
    "        list: A merged sorted list.\n",
    "    \"\"\"\n",
    "    merged = []  # Result list\n",
    "    i = j = 0    # Pointers for left and right lists\n",
    "    \n",
    "    # Merge the two lists by comparing elements\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:  # Ensure stability by using <=\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    \n",
    "    # Add any remaining elements from left and right\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsorted = [34, 7, 23, 32, 5, 62]\n",
    "unsorted = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_list = merge_sort(unsorted)\n",
    "print(sorted_list)  # Output: [5, 7, 23, 32, 34, 62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e979c5",
   "metadata": {},
   "source": [
    "# Accessing inference service\n",
    "using ollama :\n",
    "- setup docker network : \n",
    "```bash\n",
    "docker network create llmnet\n",
    "```\n",
    "- launch an ollama container \n",
    "```bash\n",
    "docker run -d  --gpus all --name ollama   --network llmnet   -p 11434:11434  ollama/ollama\n",
    "```\n",
    "\n",
    "Note: make sure that this notebook's container is also launched with `--network llmnet`\n",
    "\n",
    "- attach to the ollama container to pull the model\n",
    "```bash\n",
    "docker exec -it ollama bash\n",
    "```\n",
    "- from within the container, pull the model\n",
    "```\n",
    "ollama pull qwen3:8b\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(messages):\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            prompt += f\"System: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {msg['content']}\\n\"\n",
    "    response = requests.post(\n",
    "        \"http://ollama:11434/api/generate\",\n",
    "        json={\"model\": \"qwen3:8b\", \"prompt\": prompt}\n",
    "    )\n",
    "    print(response.status_code)\n",
    "    full_text = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode('utf-8'))\n",
    "            # The generated text is usually in the 'response' field\n",
    "            full_text += data.get(\"response\", \"\")\n",
    "    return full_text\n",
    "\n",
    "def get_response_openai(messages, model=\"qwen3:8b\", base_url=\"http://ollama:11434/v1\"):\n",
    "    # Create a client that points to the Ollama OpenAI-compatible endpoint\n",
    "    client = openai.OpenAI(\n",
    "        api_key=\"ollama\",  # Any string, Ollama doesn't check it\n",
    "        base_url=base_url\n",
    "    )\n",
    "    # Call the chat completion endpoint\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    # Extract the assistant's reply\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aceead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using native ollama api\n",
    "response = get_response(generation_chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using openai api\n",
    "response = get_response_openai(generation_chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Sorts an array using the Merge Sort algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (list): The list of elements to be sorted.\n",
    "    \n",
    "    Returns:\n",
    "    list: A new list containing all elements from the original list, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr  # Base case: single-element list is already sorted\n",
    "    \n",
    "    # Split the array into left and right halves\n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid])  # Recursively sort the left half\n",
    "    right = merge_sort(arr[mid:])  # Recursively sort the right half\n",
    "    \n",
    "    # Merge the sorted halves\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merges two sorted lists into a single sorted list.\n",
    "    \n",
    "    Parameters:\n",
    "    left (list): The first sorted list.\n",
    "    right (list): The second sorted list.\n",
    "    \n",
    "    Returns:\n",
    "    list: A new list containing all elements from both input lists, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = j = 0\n",
    "    \n",
    "    # Merge elements from both lists\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] < right[j]:\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    \n",
    "    # Add any remaining elements from the left or right list\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsorted = [34, 7, 23, 32, 5, 62]\n",
    "unsorted = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_list = merge_sort(unsorted)\n",
    "print(sorted_list)  # Output: [5, 7, 23, 32, 34, 62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f46fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Yes, I’m here and ready to help. What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://ollama:11434/v1\",\n",
    "    api_key=\"ollama\",\n",
    "    model=\"gpt-oss:20b\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Test the setup\n",
    "response = llm.invoke(\"Hello! Are you working?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be04424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a **stand‑alone, fully documented** implementation of the classic **Merge Sort** algorithm in Python.  \n",
      "It follows the divide‑and‑conquer paradigm, runs in **O(n log n)** time, and is **stable** (equal elements keep their relative order).\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "merge_sort.py\n",
      "\n",
      "A clean, recursive implementation of Merge Sort that works on any\n",
      "sequence of comparable items (lists, tuples, etc.).\n",
      "\n",
      "Author:  ChatGPT\n",
      "Date:    2025‑08‑27\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import annotations\n",
      "from typing import List, Sequence, TypeVar, Callable\n",
      "\n",
      "T = TypeVar(\"T\")  # generic type for elements in the sequence\n",
      "\n",
      "\n",
      "def merge_sort(\n",
      "    arr: Sequence[T],\n",
      "    *,\n",
      "    key: Callable[[T], any] | None = None,\n",
      "    reverse: bool = False,\n",
      ") -> List[T]:\n",
      "    \"\"\"\n",
      "    Return a new list containing the elements of *arr* sorted in ascending\n",
      "    order (or descending if reverse=True).  The original sequence is left\n",
      "    untouched.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : Sequence[T]\n",
      "        The input sequence to sort.  It can be a list, tuple, or any\n",
      "        sequence that supports indexing and len().\n",
      "    key : Callable[[T], any], optional\n",
      "        A function that extracts a comparison key from each element.\n",
      "        If omitted, the elements themselves are compared.\n",
      "    reverse : bool, default False\n",
      "        If True, sort in descending order.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    List[T]\n",
      "        A new list containing the sorted elements.\n",
      "\n",
      "    Complexity\n",
      "    ----------\n",
      "    Time   : O(n log n)   (worst, average, and best case)\n",
      "    Memory : O(n)         (auxiliary list for merging)\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    * Merge Sort is stable: equal elements retain their original order.\n",
      "    * The algorithm is recursive; for very large inputs you might hit\n",
      "      Python's recursion limit.  In that case, an iterative version\n",
      "      (see below) can be used.\n",
      "    \"\"\"\n",
      "    # Convert key function to a lambda that returns the key or the element itself\n",
      "    if key is None:\n",
      "        key = lambda x: x\n",
      "\n",
      "    # Base case: 0 or 1 element is already sorted\n",
      "    if len(arr) <= 1:\n",
      "        return list(arr)\n",
      "\n",
      "    # Divide\n",
      "    mid = len(arr) // 2\n",
      "    left = merge_sort(arr[:mid], key=key, reverse=reverse)\n",
      "    right = merge_sort(arr[mid:], key=key, reverse=reverse)\n",
      "\n",
      "    # Merge\n",
      "    merged: List[T] = []\n",
      "    i = j = 0\n",
      "    while i < len(left) and j < len(right):\n",
      "        # Compare keys; reverse order if requested\n",
      "        if (key(left[i]) < key(right[j])) ^ reverse:\n",
      "            merged.append(left[i])\n",
      "            i += 1\n",
      "        else:\n",
      "            merged.append(right[j])\n",
      "            j += 1\n",
      "\n",
      "    # Append any remaining elements\n",
      "    merged.extend(left[i:])\n",
      "    merged.extend(right[j:])\n",
      "    return merged\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------------------- #\n",
      "# Optional: an in‑place variant that reuses the input list (uses O(n) extra)\n",
      "# --------------------------------------------------------------------------- #\n",
      "def merge_sort_inplace(arr: List[T], *, key: Callable[[T], any] | None = None) -> None:\n",
      "    \"\"\"\n",
      "    Sort *arr* in place using Merge Sort.  This variant uses an auxiliary\n",
      "    list of the same size as *arr* to perform the merges, so the memory\n",
      "    overhead is still O(n).\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : List[T]\n",
      "        The list to sort.  It will be modified in place.\n",
      "    key : Callable[[T], any], optional\n",
      "        Function to extract a comparison key from each element.\n",
      "    \"\"\"\n",
      "    if key is None:\n",
      "        key = lambda x: x\n",
      "\n",
      "    def _merge_sort(start: int, end: int) -> None:\n",
      "        \"\"\"Recursively sort arr[start:end] in place.\"\"\"\n",
      "        if end - start <= 1:\n",
      "            return\n",
      "        mid = (start + end) // 2\n",
      "        _merge_sort(start, mid)\n",
      "        _merge_sort(mid, end)\n",
      "\n",
      "        # Merge arr[start:mid] and arr[mid:end] into temp\n",
      "        temp: List[T] = []\n",
      "        i, j = start, mid\n",
      "        while i < mid and j < end:\n",
      "            if key(arr[i]) <= key(arr[j]):\n",
      "                temp.append(arr[i])\n",
      "                i += 1\n",
      "            else:\n",
      "                temp.append(arr[j])\n",
      "                j += 1\n",
      "        temp.extend(arr[i:mid])\n",
      "        temp.extend(arr[j:end])\n",
      "\n",
      "        # Copy back to arr\n",
      "        arr[start:end] = temp\n",
      "\n",
      "    _merge_sort(0, len(arr))\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------------------- #\n",
      "# Example usage\n",
      "# --------------------------------------------------------------------------- #\n",
      "if __name__ == \"__main__\":\n",
      "    import random\n",
      "\n",
      "    # Generate a random list of integers\n",
      "    data = [random.randint(0, 100) for _ in range(15)]\n",
      "    print(\"Unsorted:\", data)\n",
      "\n",
      "    # Functional (returns a new sorted list)\n",
      "    sorted_data = merge_sort(data)\n",
      "    print(\"Sorted (functional):\", sorted_data)\n",
      "\n",
      "    # In‑place sorting\n",
      "    merge_sort_inplace(data)\n",
      "    print(\"Sorted (in‑place):\", data)\n",
      "\n",
      "    # Sorting with a key (e.g., sort by absolute value)\n",
      "    data = [random.randint(-50, 50) for _ in range(10)]\n",
      "    print(\"\\nUnsorted with negatives:\", data)\n",
      "    sorted_by_abs = merge_sort(data, key=abs)\n",
      "    print(\"Sorted by absolute value:\", sorted_by_abs)\n",
      "\n",
      "    # Descending order\n",
      "    print(\"Descending:\", merge_sort(data, reverse=True))\n",
      "```\n",
      "\n",
      "### How it works\n",
      "\n",
      "1. **Base case** – If the sequence has 0 or 1 element, it is already sorted.\n",
      "2. **Divide** – Split the sequence into two halves.\n",
      "3. **Conquer** – Recursively sort each half.\n",
      "4. **Combine** – Merge the two sorted halves into a single sorted list.\n",
      "\n",
      "The merge step uses two pointers (`i` and `j`) to walk through the left and right halves, picking the smaller (or larger, if `reverse=True`) element each time and appending it to the result list. Remaining elements from either half are appended once one half is exhausted.\n",
      "\n",
      "### Why use Merge Sort?\n",
      "\n",
      "- **Stable** – preserves the relative order of equal elements.\n",
      "- **Predictable** – always `O(n log n)` time, regardless of input distribution.\n",
      "- **Good for linked lists** – can be implemented without random access.\n",
      "\n",
      "Feel free to copy the code into your project or adapt it to your needs!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Generate a Python implementation of the Merge Sort algorithm\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe1eb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsorted: [43, 57, 76, 51, 6, 94, 38, 62, 46, 69, 47, 77, 36, 79, 90]\n",
      "Sorted (functional): [6, 36, 38, 43, 46, 47, 51, 57, 62, 69, 76, 77, 79, 90, 94]\n",
      "Sorted (in‑place): [6, 36, 38, 43, 46, 47, 51, 57, 62, 69, 76, 77, 79, 90, 94]\n",
      "\n",
      "Unsorted with negatives: [-14, -5, 5, 28, 16, -5, -8, -38, 12, 25]\n",
      "Sorted by absolute value: [-5, 5, -5, -8, 12, -14, 16, 25, 28, -38]\n",
      "Descending: [28, 25, 16, 12, 5, -5, -5, -8, -14, -38]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "merge_sort.py\n",
    "\n",
    "A clean, recursive implementation of Merge Sort that works on any\n",
    "sequence of comparable items (lists, tuples, etc.).\n",
    "\n",
    "Author:  ChatGPT\n",
    "Date:    2025‑08‑27\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Sequence, TypeVar, Callable\n",
    "\n",
    "T = TypeVar(\"T\")  # generic type for elements in the sequence\n",
    "\n",
    "\n",
    "def merge_sort(\n",
    "    arr: Sequence[T],\n",
    "    *,\n",
    "    key: Callable[[T], any] | None = None,\n",
    "    reverse: bool = False,\n",
    ") -> List[T]:\n",
    "    \"\"\"\n",
    "    Return a new list containing the elements of *arr* sorted in ascending\n",
    "    order (or descending if reverse=True).  The original sequence is left\n",
    "    untouched.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : Sequence[T]\n",
    "        The input sequence to sort.  It can be a list, tuple, or any\n",
    "        sequence that supports indexing and len().\n",
    "    key : Callable[[T], any], optional\n",
    "        A function that extracts a comparison key from each element.\n",
    "        If omitted, the elements themselves are compared.\n",
    "    reverse : bool, default False\n",
    "        If True, sort in descending order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[T]\n",
    "        A new list containing the sorted elements.\n",
    "\n",
    "    Complexity\n",
    "    ----------\n",
    "    Time   : O(n log n)   (worst, average, and best case)\n",
    "    Memory : O(n)         (auxiliary list for merging)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * Merge Sort is stable: equal elements retain their original order.\n",
    "    * The algorithm is recursive; for very large inputs you might hit\n",
    "      Python's recursion limit.  In that case, an iterative version\n",
    "      (see below) can be used.\n",
    "    \"\"\"\n",
    "    # Convert key function to a lambda that returns the key or the element itself\n",
    "    if key is None:\n",
    "        key = lambda x: x\n",
    "\n",
    "    # Base case: 0 or 1 element is already sorted\n",
    "    if len(arr) <= 1:\n",
    "        return list(arr)\n",
    "\n",
    "    # Divide\n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid], key=key, reverse=reverse)\n",
    "    right = merge_sort(arr[mid:], key=key, reverse=reverse)\n",
    "\n",
    "    # Merge\n",
    "    merged: List[T] = []\n",
    "    i = j = 0\n",
    "    while i < len(left) and j < len(right):\n",
    "        # Compare keys; reverse order if requested\n",
    "        if (key(left[i]) < key(right[j])) ^ reverse:\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "\n",
    "    # Append any remaining elements\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    return merged\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Optional: an in‑place variant that reuses the input list (uses O(n) extra)\n",
    "# --------------------------------------------------------------------------- #\n",
    "def merge_sort_inplace(arr: List[T], *, key: Callable[[T], any] | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Sort *arr* in place using Merge Sort.  This variant uses an auxiliary\n",
    "    list of the same size as *arr* to perform the merges, so the memory\n",
    "    overhead is still O(n).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : List[T]\n",
    "        The list to sort.  It will be modified in place.\n",
    "    key : Callable[[T], any], optional\n",
    "        Function to extract a comparison key from each element.\n",
    "    \"\"\"\n",
    "    if key is None:\n",
    "        key = lambda x: x\n",
    "\n",
    "    def _merge_sort(start: int, end: int) -> None:\n",
    "        \"\"\"Recursively sort arr[start:end] in place.\"\"\"\n",
    "        if end - start <= 1:\n",
    "            return\n",
    "        mid = (start + end) // 2\n",
    "        _merge_sort(start, mid)\n",
    "        _merge_sort(mid, end)\n",
    "\n",
    "        # Merge arr[start:mid] and arr[mid:end] into temp\n",
    "        temp: List[T] = []\n",
    "        i, j = start, mid\n",
    "        while i < mid and j < end:\n",
    "            if key(arr[i]) <= key(arr[j]):\n",
    "                temp.append(arr[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                temp.append(arr[j])\n",
    "                j += 1\n",
    "        temp.extend(arr[i:mid])\n",
    "        temp.extend(arr[j:end])\n",
    "\n",
    "        # Copy back to arr\n",
    "        arr[start:end] = temp\n",
    "\n",
    "    _merge_sort(0, len(arr))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Example usage\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "import random\n",
    "\n",
    "# Generate a random list of integers\n",
    "data = [random.randint(0, 100) for _ in range(15)]\n",
    "print(\"Unsorted:\", data)\n",
    "\n",
    "# Functional (returns a new sorted list)\n",
    "sorted_data = merge_sort(data)\n",
    "print(\"Sorted (functional):\", sorted_data)\n",
    "\n",
    "# In‑place sorting\n",
    "merge_sort_inplace(data)\n",
    "print(\"Sorted (in‑place):\", data)\n",
    "\n",
    "# Sorting with a key (e.g., sort by absolute value)\n",
    "data = [random.randint(-50, 50) for _ in range(10)]\n",
    "print(\"\\nUnsorted with negatives:\", data)\n",
    "sorted_by_abs = merge_sort(data, key=abs)\n",
    "print(\"Sorted by absolute value:\", sorted_by_abs)\n",
    "\n",
    "# Descending order\n",
    "print(\"Descending:\", merge_sort(data, reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
