{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the start of your adventure in Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Are you ready for action??</h2>\n",
    "            <span style=\"color:#ff7800;\">Have you completed all the setup steps in the <a href=\"../setup/\">setup</a> folder?<br/>\n",
    "            Have you read the <a href=\"../README.md\">README</a>? Many common questions are answered here!<br/>\n",
    "            Have you checked out the guides in the <a href=\"../guides/01_intro.ipynb\">guides</a> folder?<br/>\n",
    "            Well in that case, you're ready!!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">This code is a live resource - keep an eye out for my updates</h2>\n",
    "            <span style=\"color:#00bfff;\">I push updates regularly. As people ask questions or have problems, I add more examples and improve explanations. As a result, the code below might not be identical to the videos, as I've added more steps and better comments. Consider this like an interactive book that accompanies the lectures.<br/><br/>\n",
    "            I try to send emails regularly with important updates related to the course. You can find this in the 'Announcements' section of Udemy in the left sidebar. You can also choose to receive my emails via your Notification Settings in Udemy. I'm respectful of your inbox and always try to add value with my emails!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And please do remember to contact me if I can help\n",
    "\n",
    "And I love to connect: https://www.linkedin.com/in/eddonner/\n",
    "\n",
    "\n",
    "### New to Notebooks like this one? Head over to the guides folder!\n",
    "\n",
    "Just to check you've already added the Python and Jupyter extensions to Cursor, if not already installed:\n",
    "- Open extensions (View >> extensions)\n",
    "- Search for python, and when the results show, click on the ms-python one, and Install it if not already installed\n",
    "- Search for jupyter, and when the results show, click on the Microsoft one, and Install it if not already installed  \n",
    "Then View >> Explorer to bring back the File Explorer.\n",
    "\n",
    "And then:\n",
    "1. Click where it says \"Select Kernel\" near the top right, and select the option called `.venv (Python 3.12.9)` or similar, which should be the first choice or the most prominent choice. You may need to choose \"Python Environments\" first.\n",
    "2. Click in each \"cell\" below, starting with the cell immediately below this text, and press Shift+Enter to run\n",
    "3. Enjoy!\n",
    "\n",
    "After you click \"Select Kernel\", if there is no option like `.venv (Python 3.12.9)` then please do the following:  \n",
    "1. On Mac: From the Cursor menu, choose Settings >> VS Code Settings (NOTE: be sure to select `VSCode Settings` not `Cursor Settings`);  \n",
    "On Windows PC: From the File menu, choose Preferences >> VS Code Settings(NOTE: be sure to select `VSCode Settings` not `Cursor Settings`)  \n",
    "2. In the Settings search bar, type \"venv\"  \n",
    "3. In the field \"Path to folder with a list of Virtual Environments\" put the path to the project root, like C:\\Users\\username\\projects\\agents (on a Windows PC) or /Users/username/projects/agents (on Mac or Linux).  \n",
    "And then try again.\n",
    "\n",
    "Having problems with missing Python versions in that list? Have you ever used Anaconda before? It might be interferring. Quit Cursor, bring up a new command line, and make sure that your Anaconda environment is deactivated:    \n",
    "`conda deactivate`  \n",
    "And if you still have any problems with conda and python versions, it's possible that you will need to run this too:  \n",
    "`conda config --set auto_activate_base false`  \n",
    "and then from within the Agents directory, you should be able to run `uv python list` and see the Python 3.12 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's do an import. If you get an Import Error, double check that your Kernel is correct..\n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next it's time to load the API keys into environment variables\n",
    "# If this returns false, see the next cell!\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, did that just output `False`??\n",
    "\n",
    "If so, the most common reason is that you didn't save your `.env` file after adding the key! Be sure to have saved.\n",
    "\n",
    "Also, make sure the `.env` file is named precisely `.env` and is in the project root directory (`agents`)\n",
    "\n",
    "By the way, your `.env` file should have a stop symbol next to it in Cursor on the left, and that's actually a good thing: that's Cursor saying to you, \"hey, I realize this is a file filled with secret information, and I'm not going to send it to an external AI to suggest changes, because your keys should not be shown to anyone else.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Final reminders</h2>\n",
    "            <span style=\"color:#ff7800;\">1. If you're not confident about Environment Variables or Web Endpoints / APIs, please read Topics 3 and 5 in this <a href=\"../guides/04_technical_foundations.ipynb\">technical foundations guide</a>.<br/>\n",
    "            2. If you want to use AIs other than OpenAI, like Gemini, DeepSeek or Ollama (free), please see the first section in this <a href=\"../guides/09_ai_apis_and_ollama.ipynb\">AI APIs guide</a>.<br/>\n",
    "            3. If you ever get a Name Error in Python, you can always fix it immediately; see the last section of this <a href=\"../guides/06_python_foundations.ipynb\">Python Foundations guide</a> and follow both tutorials and exercises.<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# Check the key - if you're not using OpenAI, check whichever key you're using! Ollama doesn't need a key.\n",
    "\n",
    "import os\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set - please head to the troubleshooting guide in the setup folder\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - the all important import statement\n",
    "# If you get an import error - head over to troubleshooting in the Setup folder\n",
    "# Even for other LLM providers like Gemini, you still use this OpenAI import - see Guide 9 for why\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we'll create an instance of the OpenAI class\n",
    "# If you're not sure what it means to create an instance of a class - head over to the guides folder (guide 6)!\n",
    "# If you get a NameError - head over to the guides folder (guide 6)to learn about NameErrors - always instantly fixable\n",
    "# If you're not using OpenAI, you just need to slightly modify this - precise instructions are in the AI APIs guide (guide 9)\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of messages in the familiar OpenAI format\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 2+2?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now call it! Any problems, head to the troubleshooting guide\n",
    "# This uses GPT 5 nano, the incredibly cheap model\n",
    "# The APIs guide (guide 9) has exact instructions for using even cheaper or free alternatives to OpenAI\n",
    "# If you get a NameError, head to the guides folder (guide 6) to learn about NameErrors - always instantly fixable\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now - let's ask for a question:\n",
    "\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given 12 visually identical balls; one ball has a different weight (you do not know whether it is heavier or lighter). Using only a two-pan balance scale and exactly three weighings, how can you determine which ball is the odd one and whether it is heavier or lighter?\n"
     ]
    }
   ],
   "source": [
    "# ask it - this uses GPT 5 mini, still cheap but more powerful than nano\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "question = response.choices[0].message.content\n",
    "\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a new messages list\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask it again\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(answer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "That was a small, simple step in the direction of Agentic AI, with your new environment!\n",
    "\n",
    "Next time things get more interesting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Now try this commercial application:<br/>\n",
    "            First ask the LLM to pick a business area that might be worth exploring for an Agentic AI opportunity.<br/>\n",
    "            Then ask the LLM to present a pain-point in that industry - something challenging that might be ripe for an Agentic solution.<br/>\n",
    "            Finally have 3 third LLM call propose the Agentic AI solution. <br/>\n",
    "            We will cover this at up-coming labs, so don't worry if you're unsure.. just give it a try!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the messages:\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Something here\"}]\n",
    "\n",
    "# Then make the first call:\n",
    "\n",
    "response =\n",
    "\n",
    "# Then read the business idea:\n",
    "\n",
    "business_idea = response.\n",
    "\n",
    "# And repeat! In the next message, include the business idea within the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_idea = \"consider the problem of optimizing cuda kernels for specific GPUs. you are given a kernel that is already optimized for a specific GPU. you are given with a profiler report that shows the performance of the kernel on the target GPU. you need to optimize the kernel given the profiler report.\"\n",
    "question = \"given the following business idea, propose an agentic solution to the problem. business idea: \" + business_idea\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=messages\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a practical, agentic solution you can implement to take a CUDA kernel and a profiler report and automatically produce an optimized kernel for a specific GPU. The approach combines rule-based diagnosis (from profiler metrics) with an automated transformation/search loop (autotuning + adaptive compile/run) so the agent can both recommend quick fixes and run systematic, validated optimizations.\n",
      "\n",
      "High-level design\n",
      "- Input: source/kernel, profiler report (Nsight Compute JSON or similar), optional test harness + correctness tests, target GPU model.\n",
      "- Output: optimized kernel source or binary, bench results, changelog (what changed and why).\n",
      "- Agent type: closed-loop autotuning agent with two tiers:\n",
      "  1. Rule-based analyzer to produce prioritized suggestions from profiler metrics.\n",
      "  2. Autotuner that applies transformations, compiles, runs tests/benchmarks, and searches the parameter space using a surrogate-assisted optimizer.\n",
      "\n",
      "Components\n",
      "1. Profiler parser\n",
      "   - Parse Nsight Compute/Nsight Systems (ncu) JSON or nvprof output.\n",
      "   - Extract key metrics per kernel: achieved_occupancy, warp_execution_efficiency, warp_nonpred_execution_efficiency, sm_efficiency, gpu_time, dram_throughput, l2_hit_rate, gld/gst transactions, shared_mem_utilization, registers_per_thread, local_memory_usage (spills), instruction mix (FP/INT/LD/ST), stall reasons (memdeps/pipe_busy/others), active_warps_per_sm.\n",
      "\n",
      "2. Bottleneck analyzer (rule-based)\n",
      "   - Map metrics to causes and ranked remediation list. Example mappings:\n",
      "     - High dram throughput, low L2 hit rate, memory-bound (sm_efficiency low, memory_throughput near device peak) → optimize memory accesses: coalesce, use shared memory, use read-only cache (__ldg), use vectorized loads, reduce working set.\n",
      "     - Low achieved occupancy / high registers_per_thread / many register spills → reduce register pressure: reduce inlining, split kernel, use launch_bounds, use -maxrregcount, change thread/block configuration, restructure to use shared memory.\n",
      "     - High warp divergence / low warp_execution_efficiency → remove branches, use predication, reorganize data so threads in warp follow same path, use warp-level primitives.\n",
      "     - High shared memory bank conflicts / high shared_mem_utilization → reorganize indexing, pad to avoid bank conflicts, reduce shared usage or re-tile.\n",
      "     - High instruction issue stalls, compute-bound → improve instruction-level parallelism, use faster math intrinsics (fmadd/ __fmaf_rn), compile flags, unroll loops, fuse kernels to reduce kernel launch overhead.\n",
      "     - High kernel launch overhead or many small kernels → fuse kernels, persistent thread model, pipelining with streams.\n",
      "   - Produce prioritized “actions” with estimated impact and cost.\n",
      "\n",
      "3. Transformation generator\n",
      "   - Implements discrete transformation operators and parameterized variants. Examples:\n",
      "     - Thread/block tuning: threads_per_block in {32,64,128,256,512,1024} and grid sizes.\n",
      "     - Memory tiling sizes (tile_x, tile_y), block-wise shared memory buffering.\n",
      "     - Loop unrolling factors (1,2,4,8,16 or pragma unroll).\n",
      "     - Replace global loads by shared memory preload (double buffering).\n",
      "     - Use __ldg (read-only cache) or __restrict__ qualifiers.\n",
      "     - Use warp intrinsics (shfl, vote, ballot) instead of global synchronization.\n",
      "     - Use math intrinsics (fast math), fused multiply-add.\n",
      "     - Limit registers via -maxrregcount or launch_bounds attribute.\n",
      "     - Kernel fusion / fission.\n",
      "     - Replace atomics with privatization + reduction.\n",
      "     - Use cooperative groups or async copy (memcpy_async on Ampere+) for global->shared prefetch.\n",
      "     - Reorder data structure layouts (AoS → SoA) to improve coalescing.\n",
      "   - Each operator parameterized by ranges.\n",
      "\n",
      "4. Compiler/executor\n",
      "   - Use NVRTC or nvcc for dynamic compilation; keep consistent compile flags.\n",
      "   - Provide functional tests (unit tests, randomized inputs) to ensure correctness after transformations.\n",
      "   - Run microbenchmarks to measure kernel runtime and selected metrics (via NCU CLI) in automated fashion.\n",
      "   - Limit budgets per candidate (timeout, iterations).\n",
      "\n",
      "5. Search/optimizer\n",
      "   - Multi-stage search:\n",
      "     - Stage A: rule-based targeted tries. Apply top-k low-cost fixes suggested by analyzer (e.g., try different block sizes, enable __ldg, add -maxrregcount).\n",
      "     - Stage B: autotuning over parameterized transforms using Bayesian optimization or SMAC/BOHB/CMA-ES. Use multi-fidelity: short-run timing (few iterations) to estimate promising candidates, then longer full-run evaluation for top candidates.\n",
      "     - Surrogate model: fit a performance predictor (Gaussian process or random forest) to reduce expensive runs. Optionally use transfer learning from previous kernels on same GPU architecture.\n",
      "   - Use parallel evaluation with resource limits.\n",
      "\n",
      "6. Validator and rollback\n",
      "   - Verify correctness with regression tests for every candidate that passes performance thresholds.\n",
      "   - Keep a changelog (diffs) and performance metrics per iteration.\n",
      "   - Final selection picks best-performing candidate meeting correctness and stability constraints.\n",
      "\n",
      "7. Reporting and explanation module\n",
      "   - Produce a human-readable report: which metrics caused the change, which transformations were applied, before/after metrics, recommended permanent changes (and why).\n",
      "   - Include code diffs and commands to reproduce.\n",
      "\n",
      "Decision logic & metric thresholds (example heuristics)\n",
      "- If achieved_occupancy < 0.5 and registers_per_thread > threshold (e.g., > 64 on some GPUs) → try decreasing registers (launch_bounds, -maxrregcount), increase threads per block (if occupancy increases), or split kernel.\n",
      "- If memory_throughput > 0.7 * peak and sm_efficiency low → memory-bound; try coalescing, shared memory buffering, reduce memory traffic.\n",
      "- If global load transactions per byte is high or gld_transactions >> ideal → poor coalescing; propose reorganizing layout/AoS→SoA, aligned loads, vector loads (float2/float4).\n",
      "- If warp_execution_efficiency < 0.9 → branch/masking issues; analyze divergent branches and apply predication or data reordering.\n",
      "- If spill stores/loads significant → registers spilling to local memory; reduce register pressure.\n",
      "- If shared_memory_bank_conflicts metric high → pad arrays.\n",
      "\n",
      "Example iteration flow (pseudocode)\n",
      "1. Parse profiler report → metrics.\n",
      "2. Bottleneck analyzer → top recommendations R = [r1, r2, r3...].\n",
      "3. For each low-cost recommendation r in R:\n",
      "     - Apply code patch (or compile flag).\n",
      "     - Compile, run correctness test, run short benchmark, collect metrics.\n",
      "     - If improvement significant, keep; else revert.\n",
      "4. If still not satisfactory, start autotune:\n",
      "     - Define parameter space (thread block size, tile sizes, unroll factors, shared mem usage).\n",
      "     - Run Bayesian optimizer with multi-fidelity evaluation.\n",
      "     - Validate top candidates fully.\n",
      "5. Return best validated kernel and report.\n",
      "\n",
      "Tooling & integration suggestions\n",
      "- Profiler: Nsight Compute CLI (ncu) with JSON export.\n",
      "- Compilation: NVRTC for dynamic trials or nvcc for full builds.\n",
      "- Source transformation: a combination of:\n",
      "     - AST tools (Clang tooling) for automatic rewrites.\n",
      "     - Template-based transformations (hand-crafted code templates).\n",
      "     - Domain-specific IRs (if kernel is from a high-level op, use TVM/AutoTVM, Triton, Halide).\n",
      "- Autotuning frameworks: OpenTuner, SMAC, BOHB, or custom Bayesian optimizer with GPyTorch. For tensor ops consider TVM/AutoTVM or Ansor.\n",
      "- Surrogate modeling: random forest or Gaussian process (scikit-learn, GPyTorch).\n",
      "- CI: run agent on a small representative dataset first, then full workload.\n",
      "\n",
      "Practical quick-start checklist for the agent to try first (low-cost fixes, high chance of wins)\n",
      "1. Try several thread-block sizes: 128, 256, 512 (record occupancy).\n",
      "2. Add __restrict__ and __launch_bounds__ where appropriate.\n",
      "3. Use __ldg() on read-only arrays.\n",
      "4. Reorder memory layout (AoS→SoA) for hotspots.\n",
      "5. Try -maxrregcount (experiment with 32,48,64) to reduce spills; ensure not too low to reduce ILP.\n",
      "6. Add shared-memory tiling for hot load-heavy loops.\n",
      "7. Use pragma unroll for inner loops (factors 2/4/8).\n",
      "8. Replace expensive math with fast intrinsics where acceptable.\n",
      "9. Check for trivial divergence fixes (reorder conditionals, use masks).\n",
      "10. If many small kernels: fuse or persistent thread model.\n",
      "\n",
      "Notes on correctness and safety\n",
      "- Every automated change must run correctness tests; performance without correctness is useless.\n",
      "- Limit runtime for each candidate; maintain deterministic seeds for benchmarks.\n",
      "- Log and provide easy rollback (git-style patches).\n",
      "\n",
      "Expected outcomes & metrics to track\n",
      "- Improvements in: kernel runtime, achieved_occupancy, memory throughput reduction for memory-bound kernels, fewer global transactions, lower local-memory (spill) usage, higher warp execution efficiency.\n",
      "- Also track compile time and code complexity; sometimes slightly slower but simpler code is preferable.\n",
      "\n",
      "Example deployment scenario\n",
      "- Build the agent as a service:\n",
      "  - Input endpoint accepts kernel + baseline profiler JSON + harness.\n",
      "  - Agent returns best patch, performance delta, and human-readable explanation.\n",
      "- Allow developer to override/approve transformations.\n",
      "- Keep a database of past optimizations per GPU model to enable transfer learning.\n",
      "\n",
      "Wrap-up\n",
      "This agentic solution pairs a metrics-driven diagnosis with an automated, validated search-and-apply pipeline. Start with a rule-based analyzer to get quick wins and then escalate to an autotuner with surrogate modeling for higher-payoff, riskier transformations. Use Nsight Compute metrics as the core signals, ensure correctness tests, and keep a transparent changelog so developers can understand and accept automated changes.\n"
     ]
    }
   ],
   "source": [
    "answer = response.choices[0].message.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
